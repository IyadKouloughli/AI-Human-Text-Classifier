# AI-Human-Text-Classifier

A lightweight DistilBERT model for detecting whether a given text is written by a human or generated by AI. The model is trained on a balanced subset of the `AI_Human.csv` dataset and achieves **99% accuracy** on the test set.

## Dataset Overview
The dataset `AI_Human.csv` contains 487,235 samples with two columns:
- `text`: The input text (either human-written or AI-generated).
- `generated`: A binary label where:
  - `0` = Human-written
  - `1` = AI-generated.

The dataset is available at: [Kaggle AI vs Human Text Dataset](https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text).

### Sampling Strategy
Due to the large size of the dataset, we balanced it by sampling **10,000 texts from each class** (total 20,000 samples) to ensure efficient training without compromising performance.

---

## Code Workflow

### 1. Data Loading and Preprocessing
- Loaded `AI_Human.csv` and sampled 10,000 texts per class.
- Removed NaN values and ensured labels are integers.
- Saved the preprocessed subset as `sampled_preprocessed_data.csv`.

### 2. Train-Validation-Test Split
- Split the data into:
  - **Train (80%)**: 16,000 samples.
  - **Validation (10%)**: 2,000 samples.
  - **Test (10%)**: 2,000 samples.
- Used stratification to maintain class balance.

### 3. Tokenization
- Tokenized text using the `DistilBertTokenizer` with:
  - Maximum sequence length: 512.
  - Padding and truncation for uniform input size.

### 4. Model Training
- Initialized `DistilBertForSequenceClassification` with 2 labels (Human/AI).
- Trained for **3 epochs** with:
  - Batch size: 16.
  - Weight decay: 0.01.
  - Warmup steps: 200.
- Achieved **99.25% validation accuracy**.

### 5. Evaluation
- Test set performance:
  - **Accuracy: 99%**.
  - Precision/Recall/F1-score: 0.99 for both classes (Human and AI).

### 6. Inference
- The saved model (`best_distilbert_model`) can classify new text with confidence scores. Example:
  ```python
  Predicted class: Human
  Confidence: 98.59%
  Probabilities - Human: 98.59%, AI: 1.41%
  ```

## How to Use
Clone the repository:
```bash
git clone https://github.com/your-username/AI-Human-Text-Classifier.git
```

Install dependencies:
```bash
pip install transformers torch pandas datasets scikit-learn
```

Run the Jupyter notebook `distilbert.ipynb` to train or evaluate the model.

For inference, load the pretrained model:
```python
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
model = DistilBertForSequenceClassification.from_pretrained("best_distilbert_model")
```

## Results
| Epoch | Training Loss | Validation Loss | Accuracy  |
|-------|---------------|-----------------|-----------|
| 1     | 0.0456        | 0.0955          | 98.05%    |
| 2     | 0.0002        | 0.0478          | 99.10%    |
| 3     | 0.0002        | 0.0445          | 99.25%    |

**Test Set Performance:**
- **Accuracy**: 99%

**Classification Report:**
```
            precision  recall  f1-score  support
    Human      1.00      0.98      0.99      1000
       AI      0.98      1.00      0.99      1000
```

## License
This project is licensed under the MIT License.